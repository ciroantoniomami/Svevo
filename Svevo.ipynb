{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Svevo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEps_ba0DbZM"
      },
      "source": [
        "# General\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel, KeyedVectors\n",
        "from gensim.models.wrappers import LdaMallet\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "# Lemmatization\n",
        "import spacy\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "KMD_Lf3eC8ZI",
        "outputId": "d368125b-fe4f-4953-ae23-4e14dc0a58fc"
      },
      "source": [
        "data = pd.read_csv(\"~/Desktop/Svevo/carteggio.svevo3.csv\", sep=';', parse_dates=['date'])\n",
        "nlp_it = spacy.load(\"it\", disable=['parser', 'ner'])\n",
        "stop_words = set(['schmitz', 'signore', 'signora', 'ettore', 'lettera', 'parola', 'fare', 'cosa'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, nlp, stop_words = [], pos = ['PROPN', 'NOUN', 'VERB', 'ADJ']):\n",
        "    \"\"\"\n",
        "    Performs preprocessing on a text through spaCy, langauge based on nlp parameter.\n",
        "    Filter default stopword plus those in the list stop_words,\n",
        "    accepts only word with POS tag in the pos list,\n",
        "    filters out non-alpha words, performs lemmatization.\n",
        "    Returns a list of lemmatized, filtered tokens for the text.\n",
        "    \"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.lemma_ not in stop_words and token.pos_ in pos and not token.is_stop and token.is_alpha:\n",
        "            tokens.append(token.lemma_)\n",
        "    return tokens\n",
        "\n",
        "def create_lda_model(dictionary, corpus, num_topics, num_words = 20, passes = 20):\n",
        "    \"\"\"\n",
        "    Creates an LDA model using gensim.\n",
        "    Dictionary and corpus should be obtained with corpora.\n",
        "    Number of topics and number of passes should be tweaked depending on context.\n",
        "    \"\"\"\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes, random_state=42)\n",
        "    for topic,words in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=num_words):\n",
        "        print(str(topic)+ \": \" + words + \"\\n\")\n",
        "    return lda_model\n",
        "\n",
        "def get_lda_keywords(model, num_words = 20):\n",
        "    \"\"\"\n",
        "    Extract the firt num_words keywords from all topics of a gensim LDA model.\n",
        "    \"\"\"\n",
        "    num_topics = len(model.print_topics())\n",
        "    return [[tup2[0] for tup2 in tup[1]]for tup in model.show_topics(formatted=False, num_topics=num_topics, num_words=num_words)]\n",
        "    \n",
        "def create_paper_points_data(lda_model, corpus, num_topics):\n",
        "    \"\"\"\n",
        "    Creates a dataframe with topic scores for each text in corpus using a gensim LDA model.\n",
        "    \"\"\"\n",
        "    x = [[] for i in range(num_topics)]\n",
        "    for text in corpus:\n",
        "        l_tup = lda_model[text]\n",
        "        for val in range(num_topics):\n",
        "            added = False\n",
        "            for tup in l_tup:\n",
        "                if tup[0] == val:\n",
        "                    x[val].append(tup[1])\n",
        "                    added = True\n",
        "            if added == False:\n",
        "                x[val].append(0)\n",
        "    df = pd.DataFrame(x)\n",
        "    df = df.transpose()\n",
        "    df.columns = ['Topic' + str(n) for n in range(num_topics)]\n",
        "    return df\n",
        "\n",
        "def compute_silhouette(lda_model, corpus, num_topics):\n",
        "    \"\"\"\n",
        "    Computes silhouette index for an LDA model based on topics it classified.\n",
        "    \"\"\"\n",
        "    points = create_paper_points_data(lda_model, corpus, num_topics).values\n",
        "    lda_labels = points.argmax(axis=1) \n",
        "    return silhouette_score(points, lda_labels)\n",
        "    \n",
        "def compute_perplexity(lda_model, corpus):\n",
        "    \"\"\"\n",
        "    Computes perplexity score for an LDA model.\n",
        "    \"\"\"\n",
        "    perplexity = lda_model.log_perplexity(corpus)\n",
        "    return perplexity\n",
        "\n",
        "def compute_coherence(lda_model, texts, corpus, dictionary, coherence = \"c_v\"):\n",
        "    \"\"\"\n",
        "    Computes coherence score for an LDA model.\n",
        "    \"\"\"\n",
        "    score = CoherenceModel(model=lda_model, texts=texts, corpus=corpus, dictionary=dictionary, coherence=coherence).get_coherence()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_it = data[data.mainLanguage == \"ITA\"]\n",
        "texts_it = [preprocess_text(text, nlp_it, stop_words=stop_words, pos = ['PROPN', 'NOUN']) for text in data_it[\"text\"]]\n",
        "d_it = corpora.Dictionary(texts_it)\n",
        "d_it.filter_extremes(no_below=5, no_above=0.5)\n",
        "c_it = [d_it.doc2bow(text) for text in texts_it]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}